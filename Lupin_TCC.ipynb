{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d625a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando addins necessarios\n",
    "\n",
    "import pandas as pd\n",
    "import pageviewapi\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Importando lista de especies e escolhendo colunas de interesse#\n",
    "\n",
    "columns_to_keep = [\"nome_popular\",\"order\",\"family\",\"species\",\"author_name\"]\n",
    "\n",
    "\n",
    "rodentia_sp = pd.read_csv (\"Scopus_Rodentia.csv\", sep =\",\", usecols=columns_to_keep)\n",
    "\n",
    "\n",
    "#retirando espaco e incluindo underline como separador\n",
    "\n",
    "col= \"species\"\n",
    "\n",
    "rodentia_sp[col]= rodentia_sp[col].str.rstrip().str.replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "#Fazendo uma lista com as especies e definindo as datas\n",
    "\n",
    "R_lista_sp= rodentia_sp[\"species\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a2b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                species  year  views\n",
      "0          Galea_spixii  2015    332\n",
      "1          Galea_spixii  2016    777\n",
      "2          Galea_spixii  2017    626\n",
      "3          Galea_spixii  2018    612\n",
      "4          Galea_spixii  2019    557\n",
      "..                  ...   ...    ...\n",
      "380  Trinomys_yonenagae  2026     22\n",
      "381    Calomys_mattevii  2023     55\n",
      "382    Calomys_mattevii  2024    188\n",
      "383    Calomys_mattevii  2025    187\n",
      "384    Calomys_mattevii  2026     12\n",
      "\n",
      "[385 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Criando codigo para Pageviews\n",
    "data_inicio = \"20150701\"\n",
    "data_fim = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "def puxar_pageviews_annual(lista_sp, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Pulls annual Wikipedia pageviews for each species.\n",
    "    Retries species with no data found.\n",
    "    \"\"\"\n",
    "    pageviews_list = []\n",
    "    retry_list = []\n",
    "    \n",
    "    # First attempt\n",
    "    for sp in lista_sp:\n",
    "        try:\n",
    "            views = pageviewapi.per_article('en.wikipedia', sp, start=start_date, end=end_date, access='all-access', agent='all-agents', granularity='monthly')\n",
    "            \n",
    "            if views['items']:\n",
    "                # Aggregate by year\n",
    "                annual_views = {}\n",
    "                for item in views['items']:\n",
    "                    year = item['timestamp'][:4]\n",
    "                    annual_views[year] = annual_views.get(year, 0) + item['views']\n",
    "                \n",
    "                for year, total_views in annual_views.items():\n",
    "                    pageviews_list.append({'species': sp, 'year': int(year), 'views': total_views})\n",
    "            else:\n",
    "                retry_list.append(sp)\n",
    "        except:\n",
    "            retry_list.append(sp)\n",
    "    \n",
    "    # Retry species with no data\n",
    "    for sp in retry_list:\n",
    "        try:\n",
    "            views = pageviewapi.per_article('pt.wikipedia', sp, start='20150701', end=end_date, access='all-access', agent='all-agents', granularity='monthly')\n",
    "            if views['items']:\n",
    "                annual_views = {}\n",
    "                for item in views['items']:\n",
    "                    year = item['timestamp'][:4]\n",
    "                    annual_views[year] = annual_views.get(year, 0) + item['views']\n",
    "                \n",
    "                for year, total_views in annual_views.items():\n",
    "                    pageviews_list.append({'species': sp, 'year': int(year), 'views': total_views})\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return pd.DataFrame(pageviews_list)\n",
    "Rodentia_pageviews_df = puxar_pageviews_annual(R_lista_sp, data_inicio, data_fim)\n",
    "print(Rodentia_pageviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe8b29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo: rodentia_pageviews_annual.csv\n",
      "Total registros: 420\n"
     ]
    }
   ],
   "source": [
    "#Criando tabela completa com todas as esp√©cies e anos\n",
    "# Create all combinations of species and years\n",
    "complete_df = pd.DataFrame([(sp, year) for sp in R_lista_sp for year in range(2015, 2027)], \n",
    "                           columns=['species', 'year'])\n",
    "\n",
    "# Merge with actual data\n",
    "complete_df = complete_df.merge(Rodentia_pageviews_df, on=['species', 'year'], how='left')\n",
    "\n",
    "# Identify species with no data\n",
    "species_no_data = [sp for sp in R_lista_sp if sp not in Rodentia_pageviews_df['species'].unique()]\n",
    "\n",
    "# Fill values: N/A for no page, -- for no views that year\n",
    "complete_df['views'] = complete_df.apply(\n",
    "    lambda row: 'N/A' if row['species'] in species_no_data \n",
    "    else ('--' if pd.isna(row['views']) else int(row['views'])), axis=1)\n",
    "\n",
    "# Sort and save\n",
    "complete_df = complete_df.sort_values(['species', 'year']).reset_index(drop=True)\n",
    "complete_df.to_csv('rodentia_pageviews_annual.csv', index=False)\n",
    "print(f\"Arquivo salvo: rodentia_pageviews_annual.csv\\nTotal registros: {len(complete_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
